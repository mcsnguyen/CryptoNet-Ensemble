{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, AveragePooling2D, MaxPooling2D, Dense, Flatten\n",
    "from keras import layers, backend, utils, optimizers, datasets\n",
    "from keras.models import Model, load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_path = 'MNIST-28x28-train.txt'\n",
    "test_input_path = 'MNIST-28x28-test.txt'\n",
    "preprocessed_input_path = 'Preprocessed-MNIST-train_test.npz'\n",
    "model_dir = 'Squared/' \n",
    "weight_path = ''\n",
    "train_dataset_size = 60000\n",
    "test_dataset_size = 10000\n",
    "image_size = 784  # 28 x 28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MNIST\n",
    "def MNIST_reader(input_name, data_size_num, image_size_num):\n",
    "    content = []\n",
    "    with open(input_name, 'r') as file:\n",
    "        for line in file:\n",
    "            content.append(line)\n",
    "\n",
    "    input_pixels = np.zeros([data_size_num, image_size_num])\n",
    "    labels = np.zeros([data_size_num])\n",
    "\n",
    "    for image in range(0, len(content)):\n",
    "        coordinates = np.asarray(content[image].split('\\t'))\n",
    "        for coordinate in coordinates[2:]:\n",
    "            if coordinate == '':\n",
    "                break\n",
    "            pair = coordinate.split(':')\n",
    "            input_pixels[image, int(pair[0])] = pair[1]\n",
    "            labels[image] = coordinates[0]\n",
    "    \n",
    "    return input_pixels, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     1,
     10,
     36,
     42,
     45,
     50,
     64,
     76,
     94,
     110
    ]
   },
   "outputs": [],
   "source": [
    "# Upper and lower padding\n",
    "def ULpadding(data_list):\n",
    "    padded = []\n",
    "    for dataset in data_list:\n",
    "        reshaped = dataset.reshape(dataset.shape[0], 28, 28, 1)\n",
    "        padded_reshaped = np.insert(reshaped, 0, 0, axis=1)\n",
    "        padded.append(np.insert(padded_reshaped, 0, 0, axis=2)/255)\n",
    "    return (*padded,)\n",
    "\n",
    "# Converts integer labels to one hot encoding\n",
    "def one_hot(label_list):\n",
    "    encoded = []\n",
    "    for label in label_list:\n",
    "        encoded.append(utils.to_categorical(label))\n",
    "    return (*encoded,)\n",
    "\n",
    "# CryptoNet's Testing Network\n",
    "def Simplified_CryptoNet(input_size, custom):\n",
    "    input_shape = Input(shape=(input_size))\n",
    "    if custom:\n",
    "        conv1 = Conv2D(filters=5, kernel_size=(5,5), strides=(2,2), activation=approxReLU)(input_shape)\n",
    "    else:\n",
    "        conv1 = Conv2D(filters=5, kernel_size=(5,5), strides=(2,2), activation='relu')(input_shape)\n",
    "    \n",
    "    flat = Flatten()(conv1) \n",
    "    \n",
    "    if custom:\n",
    "        dense = Dense(100, activation=approxReLU)(flat)\n",
    "    else:\n",
    "        dense = Dense(100, activation='relu')(flat)\n",
    "        \n",
    "    output_shape = Dense(10, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=input_shape, outputs=output_shape)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def approxReLU(x):\n",
    "    return 0.125 * x**2 + 0.25 * x + 0.5\n",
    "    #return x**2\n",
    "    \n",
    "def train(model, train_images, train_labels, epoch=10, val_split=0.2, **kwargs):\n",
    "    sgd = optimizers.SGD(lr=0.1)\n",
    "    adam = optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['acc'])\n",
    "    model.fit(x=train_images, y=train_labels, epochs=epoch, verbose=1, validation_split=val_split)\n",
    "\n",
    "def avg_ensemble(predicted1, predicted2, predicted3):\n",
    "    return (predicted1 + predicted2 + predicted3)/3\n",
    "\n",
    "def max_ensemble(predicted1, predicted2, predicted3):\n",
    "    maximum = np.maximum(predicted1, predicted2)\n",
    "    return np.maximum(maximum, predicted3)\n",
    "\n",
    "# Splits same training for testing\n",
    "def with_train_data(data_list, label_list):\n",
    "    train_x1, test_x1, train_y1, test_y1 = train_test_split(datalist[1], label_list[1], test_size=0.2)\n",
    "    train_x2, test_x2, train_y2, test_y2 = train_test_split(datalist[2], label_list[2], test_size=0.2)\n",
    "    train_x3, test_x3, train_y3, test_y3 = train_test_split(datalist[3], label_list[3], test_size=0.2)\n",
    "\n",
    "    test_images = np.concatenate(test_x1, test_x2, test_x3, axis=0)\n",
    "    test_labels = np.concatenate(test_y1, test_y2, test_y3, axis=0)\n",
    "    \n",
    "    train_list = [data_list[0], train_x1, train_x2, train_x3, test_images]\n",
    "    labels_list = [label_list[0], train_y1, train_y2, train_y3, test_labels]\n",
    "    \n",
    "    return with_train_and_test_data(train_list, labels_list)\n",
    "\n",
    "# Uses different sets for training and testing\n",
    "def with_train_and_test_data(data_list, label_list, epoch):\n",
    "    whole_train, train_x1, train_x2, train_x3, test_images = ULpadding(data_list)\n",
    "    whole_label, train_y1, train_y2, train_y3, test_labels = one_hot(label_list)\n",
    "    \n",
    "    train_list = [whole_train, train_x1, train_x2, train_x3, test_images]\n",
    "    labels_list = [whole_label, train_y1, train_y2, train_y3, test_labels]\n",
    "    epoch_list = epoch_ratios(epoch, train_list) \n",
    "    \n",
    "    return epoch_list, train_list, labels_list\n",
    "    \n",
    "# Creates ratios for epoch such that smaller datasets are traversed \n",
    "# more than larger datasets to compensate\n",
    "def epoch_ratios(epoch, train_shapes):\n",
    "    ratio_list = []\n",
    "    \n",
    "    whole_shape = train_shapes[0].shape[0]\n",
    "    \n",
    "    ratio1 = whole_shape/train_shapes[1].shape[0]\n",
    "    ratio2 = whole_shape/train_shapes[2].shape[0]\n",
    "    ratio3 = whole_shape/train_shapes[3].shape[0]\n",
    "    \n",
    "    epoch_whole = epoch\n",
    "    \n",
    "    ratio_list.append(epoch_whole) # whole dataset\n",
    "    ratio_list.append(int(ratio1 * epoch_whole)) # model 1\n",
    "    ratio_list.append(int(ratio2 * epoch_whole)) # model 2\n",
    "    ratio_list.append(int(ratio3 * epoch_whole)) # model 3\n",
    "    \n",
    "    return ratio_list   \n",
    "\n",
    "def train_dataset(epoch_list, img_list, label_list, custom=False):\n",
    "    # Whole Dataset\n",
    "    model = Simplified_CryptoNet(img_list[0].shape[1:], custom)\n",
    "    train(model, img_list[0], label_list[0], epoch=epoch_list[0])\n",
    "\n",
    "    # Ensemble\n",
    "    model1 = Simplified_CryptoNet(img_list[1].shape[1:], custom)\n",
    "    model2 = Simplified_CryptoNet(img_list[2].shape[1:], custom)\n",
    "    model3 = Simplified_CryptoNet(img_list[3].shape[1:], custom)\n",
    "\n",
    "    train(model1, img_list[1], label_list[1], epoch=epoch_list[1])\n",
    "    train(model2, img_list[2], label_list[2], epoch=epoch_list[2])\n",
    "    train(model3, img_list[3], label_list[3], epoch=epoch_list[3])\n",
    "    \n",
    "    return model, model1, model2, model3\n",
    "\n",
    "def accuracy_test(model_list, test_img, test_label):\n",
    "    whole_acc = 0\n",
    "    avg_ensemble_acc = 0\n",
    "    max_ensemble_acc = 0\n",
    "    model1_acc = 0\n",
    "    model2_acc = 0\n",
    "    model3_acc = 0\n",
    "\n",
    "    whole = model_list[0].predict(test_img)\n",
    "    predict1 = model_list[1].predict(test_img)\n",
    "    predict2 = model_list[2].predict(test_img)\n",
    "    predict3 = model_list[3].predict(test_img)\n",
    "\n",
    "    avg = avg_ensemble(predict1, predict2, predict3)\n",
    "    maximum = max_ensemble(predict1, predict2, predict3)  \n",
    "    \n",
    "    true_label = np.argmax(test_label, axis=1)\n",
    "    whole_label = np.argmax(whole, axis=1)\n",
    "    avg_label = np.argmax(avg, axis=1)\n",
    "    max_label = np.argmax(maximum, axis=1)\n",
    "    model1 = np.argmax(predict1, axis=1)\n",
    "    model2 = np.argmax(predict2, axis=1)\n",
    "    model3 = np.argmax(predict3, axis=1)\n",
    "    \n",
    "    for index in range(0, test_img.shape[0]):\n",
    "        if whole_label[index] == true_label[index]:\n",
    "            whole_acc += 1\n",
    "        if avg_label[index] == true_label[index]:\n",
    "            avg_ensemble_acc += 1\n",
    "        if max_label[index] == true_label[index]:\n",
    "            max_ensemble_acc += 1\n",
    "        if model1[index] == true_label[index]:\n",
    "            model1_acc += 1\n",
    "        if model2[index] == true_label[index]:\n",
    "            model2_acc += 1\n",
    "        if model3[index] == true_label[index]:\n",
    "            model3_acc += 1\n",
    "    \n",
    "    return test_img.shape[0], whole_acc, avg_ensemble_acc, max_ensemble_acc, model1_acc, model2_acc, model3_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Read in input\n",
    "train_input_pixels, train_labels = MNIST_reader(train_input_path, train_dataset_size, image_size)\n",
    "test_input_pixels, test_labels = MNIST_reader(test_input_path, test_dataset_size, image_size)\n",
    "\n",
    "np.savez(preprocessed_input_path, train_img=train_input_pixels, train_lab=train_labels,\n",
    "        test_img=test_input_pixels, test_lab=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load in saved preprocessed np arrays\n",
    "with np.load(preprocessed_input_path, 'r') as preprocessed:\n",
    "    train_input_pixels = preprocessed['train_img']\n",
    "    train_labels = preprocessed['train_lab']\n",
    "    test_input_pixels = preprocessed['test_img']\n",
    "    test_labels = preprocessed['test_lab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]\n"
     ]
    }
   ],
   "source": [
    "# MNIST Training Splicing\n",
    "zero_images = []\n",
    "zero_labels = []\n",
    "one_images = []\n",
    "one_labels = []\n",
    "two_images = []\n",
    "two_labels = []\n",
    "three_images = []\n",
    "three_labels = []\n",
    "four_images = []\n",
    "four_labels = []\n",
    "five_images = []\n",
    "five_labels = []\n",
    "six_images = []\n",
    "six_labels = []\n",
    "seven_images = []\n",
    "seven_labels = []\n",
    "eight_images = []\n",
    "eight_labels = []\n",
    "nine_images = []\n",
    "nine_labels = []\n",
    "\n",
    "zero = []\n",
    "one = []\n",
    "two = []\n",
    "three = []\n",
    "four = []\n",
    "five = []\n",
    "six = []\n",
    "seven = []\n",
    "eight = []\n",
    "nine = []\n",
    "\n",
    "for idx in range(train_dataset_size):\n",
    "    if train_labels[idx] == 0:\n",
    "        zero_images.append(train_input_pixels[idx])\n",
    "        zero_labels.append(train_labels[idx])\n",
    "        zero.append((train_input_pixels[idx],train_labels[idx]))\n",
    "    elif train_labels[idx] == 1:\n",
    "        one_images.append(train_input_pixels[idx])\n",
    "        one_labels.append(train_labels[idx])\n",
    "        one.append((train_input_pixels[idx],train_labels[idx]))\n",
    "    elif train_labels[idx] == 2:\n",
    "        two_images.append(train_input_pixels[idx])\n",
    "        two_labels.append(train_labels[idx])\n",
    "        two.append((train_input_pixels[idx],train_labels[idx]))\n",
    "    elif train_labels[idx] == 3:\n",
    "        three_images.append(train_input_pixels[idx])\n",
    "        three_labels.append(train_labels[idx])\n",
    "        three.append((train_input_pixels[idx],train_labels[idx]))\n",
    "    elif train_labels[idx] == 4:\n",
    "        four_images.append(train_input_pixels[idx])\n",
    "        four_labels.append(train_labels[idx])\n",
    "        four.append((train_input_pixels[idx],train_labels[idx]))\n",
    "    elif train_labels[idx] == 5:\n",
    "        five_images.append(train_input_pixels[idx])\n",
    "        five_labels.append(train_labels[idx])\n",
    "        five.append((train_input_pixels[idx],train_labels[idx]))\n",
    "    elif train_labels[idx] == 6:\n",
    "        six_images.append(train_input_pixels[idx])\n",
    "        six_labels.append(train_labels[idx])\n",
    "        six.append((train_input_pixels[idx],train_labels[idx]))\n",
    "    elif train_labels[idx] == 7:\n",
    "        seven_images.append(train_input_pixels[idx])\n",
    "        seven_labels.append(train_labels[idx])\n",
    "        seven.append((train_input_pixels[idx],train_labels[idx]))\n",
    "    elif train_labels[idx] == 8:\n",
    "        eight_images.append(train_input_pixels[idx])\n",
    "        eight_labels.append(train_labels[idx])\n",
    "        eight.append((train_input_pixels[idx],train_labels[idx]))\n",
    "    else:\n",
    "        nine_images.append(train_input_pixels[idx])\n",
    "        nine_labels.append(train_labels[idx])\n",
    "        nine.append((train_input_pixels[idx],train_labels[idx]))\n",
    "\n",
    "num = [len(zero), len(one), len(two), len(three), len(four), len(five), len(six), len(seven), len(eight), len(nine)]\n",
    "print(num)\n",
    "\n",
    "dataset1_images = []\n",
    "dataset1_labels = []\n",
    "dataset2_images = []\n",
    "dataset2_labels = []\n",
    "dataset3_images = []\n",
    "dataset3_labels = []\n",
    "\n",
    "dataset1_images += zero_images[:5000]\n",
    "dataset1_images += one_images[:5000]\n",
    "dataset1_images += two_images[:5000]\n",
    "dataset1_images += three_images[:250]\n",
    "dataset1_images += four_images[:250]\n",
    "dataset1_images += five_images[:150]\n",
    "dataset1_images += six_images[:250]\n",
    "dataset1_images += seven_images[:250]\n",
    "dataset1_images += eight_images[:250]\n",
    "dataset1_images += nine_images[:250]\n",
    "\n",
    "dataset1_labels += zero_labels[:5000]\n",
    "dataset1_labels += one_labels[:5000]\n",
    "dataset1_labels += two_labels[:5000]\n",
    "dataset1_labels += three_labels[:250]\n",
    "dataset1_labels += four_labels[:250]\n",
    "dataset1_labels += five_labels[:150]\n",
    "dataset1_labels += six_labels[:250]\n",
    "dataset1_labels += seven_labels[:250]\n",
    "dataset1_labels += eight_labels[:250]\n",
    "dataset1_labels += nine_labels[:250]\n",
    "\n",
    "dataset2_images += zero_images[5000:5250]\n",
    "dataset2_images += one_images[5000:5250]\n",
    "dataset2_images += two_images[5000:5250]\n",
    "dataset2_images += three_images[250:5250]\n",
    "dataset2_images += four_images[250:5250]\n",
    "dataset2_images += five_images[150:5021]\n",
    "dataset2_images += six_images[250:500]\n",
    "dataset2_images += seven_images[250:500]\n",
    "dataset2_images += eight_images[250:500]\n",
    "dataset2_images += nine_images[250:500]\n",
    "\n",
    "dataset2_labels += zero_labels[5000:5250]\n",
    "dataset2_labels += one_labels[5000:5250]\n",
    "dataset2_labels += two_labels[5000:5250]\n",
    "dataset2_labels += three_labels[250:5250]\n",
    "dataset2_labels += four_labels[250:5250]\n",
    "dataset2_labels += five_labels[150:5021]\n",
    "dataset2_labels += six_labels[250:500]\n",
    "dataset2_labels += seven_labels[250:500]\n",
    "dataset2_labels += eight_labels[250:500]\n",
    "dataset2_labels += nine_labels[250:500]\n",
    "\n",
    "dataset3_images += zero_images[5250:]\n",
    "dataset3_images += one_images[5250:5950]\n",
    "dataset3_images += two_images[5250:5950]\n",
    "dataset3_images += three_images[5250:5950]\n",
    "dataset3_images += four_images[5250:]\n",
    "dataset3_images += five_images[5021:]\n",
    "dataset3_images += six_images[500:4300]\n",
    "dataset3_images += seven_images[500:4300]\n",
    "dataset3_images += eight_images[500:4300]\n",
    "dataset3_images += nine_images[500:4300]\n",
    "\n",
    "dataset3_labels += zero_labels[5250:]\n",
    "dataset3_labels += one_labels[5250:5950]\n",
    "dataset3_labels += two_labels[5250:5950]\n",
    "dataset3_labels += three_labels[5250:5950]\n",
    "dataset3_labels += four_labels[5250:]\n",
    "dataset3_labels += five_labels[5021:]\n",
    "dataset3_labels += six_labels[500:4300]\n",
    "dataset3_labels += seven_labels[500:4300]\n",
    "dataset3_labels += eight_labels[500:4300]\n",
    "dataset3_labels += nine_labels[500:4300]\n",
    "\n",
    "combined1 = list(zip(dataset1_images, dataset1_labels))\n",
    "combined2 = list(zip(dataset2_images, dataset2_labels))\n",
    "combined3 = list(zip(dataset3_images, dataset3_labels))\n",
    "\n",
    "random.shuffle(combined1)\n",
    "random.shuffle(combined2)\n",
    "random.shuffle(combined3)\n",
    "\n",
    "dataset1_images[:], dataset1_labels[:] = zip(*combined1)\n",
    "dataset2_images[:], dataset2_labels[:] = zip(*combined2)\n",
    "dataset3_images[:], dataset3_labels[:] = zip(*combined3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     3,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess and group data and label\n",
    "'''\n",
    "# Lists to Split Training for Testing\n",
    "data_list = [np.asarray(train_input_pixels), np.asarray(dataset1_images), np.asarray(dataset2_images),\n",
    "             np.asarray(dataset3_images)]\n",
    "label_list = [train_labels, dataset1_labels, dataset2_labels, dataset3_labels]\n",
    "\n",
    "epoch_list, train_list, label_list = with_train_data(data_list, label_list)\n",
    "'''\n",
    "\n",
    "# Lists with Training and Testing Available\n",
    "data_list = [np.asarray(train_input_pixels), np.asarray(dataset1_images), np.asarray(dataset2_images),\n",
    "             np.asarray(dataset3_images), np.asarray(test_input_pixels)]\n",
    "label_list = [train_labels, dataset1_labels, dataset2_labels, dataset3_labels, test_labels]\n",
    "\n",
    "# Add preprocessed epochs, padded, one hot encoded into lists\n",
    "# 3rd arg is epoch for whole dataset\n",
    "epoch_list, train_list, label_list = with_train_and_test_data(data_list, label_list, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load in trained model\n",
    "model_list = [] \n",
    "model_list.append(load_model(model_dir + 'model_whole.h5', custom_objects={'approxReLU': approxReLU}))\n",
    "model_list.append(load_model(model_dir + 'model1.h5', custom_objects={'approxReLU': approxReLU}))\n",
    "model_list.append(load_model(model_dir + 'model2.h5', custom_objects={'approxReLU': approxReLU}))\n",
    "model_list.append(load_model(model_dir + 'model3.h5', custom_objects={'approxReLU': approxReLU}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 17:42:14.122971  3212 deprecation_wrapper.py:119] From C:\\Users\\PandaBear\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0728 17:42:14.150969  3212 deprecation_wrapper.py:119] From C:\\Users\\PandaBear\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0728 17:42:14.158969  3212 deprecation_wrapper.py:119] From C:\\Users\\PandaBear\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0728 17:42:14.252972  3212 deprecation_wrapper.py:119] From C:\\Users\\PandaBear\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0728 17:42:14.258970  3212 deprecation_wrapper.py:119] From C:\\Users\\PandaBear\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0728 17:42:14.350969  3212 deprecation.py:323] From C:\\Users\\PandaBear\\Anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 29, 29, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 5)         130       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 845)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               84600     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 85,740\n",
      "Trainable params: 85,740\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 17:42:14.461971  3212 deprecation_wrapper.py:119] From C:\\Users\\PandaBear\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n\t [[metrics/acc/Mean/_81]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a87d5df84719>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train whole, model1, model2, model3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-043796ac35ac>\u001b[0m in \u001b[0;36mtrain_dataset\u001b[1;34m(epoch_list, img_list, label_list, custom)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m# Whole Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimplified_CryptoNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;31m# Ensemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-043796ac35ac>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_images, train_labels, epoch, val_split, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0madam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mavg_ensemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n\t [[metrics/acc/Mean/_81]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "# Train whole, model1, model2, model3\n",
    "model_list = train_dataset(epoch_list, train_list, label_list, custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_list[0].save(model_dir + 'model_whole.h5')\n",
    "model_list[1].save(model_dir + 'model1.h5')\n",
    "model_list[2].save(model_dir + 'model2.h5')\n",
    "model_list[3].save(model_dir + 'model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain accuracy parameters to print\n",
    "test_size, whole_acc, avg_ensemble_acc, max_ensemble_acc, model1_acc, model2_acc, model3_acc = accuracy_test(model_list, train_list[4], label_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire Dataset Acc: 9801/10000\n",
      "Avg Ensemble Acc: 9684/10000\n",
      "Max Ensemble Acc: 9757/10000\n",
      "Model 1 Acc: 9182/10000\n",
      "Model 2 Acc: 9381/10000\n",
      "Model 3 Acc: 9584/10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Entire Dataset Acc: \" + str(whole_acc) + \"/\" + str(test_size) +\n",
    "  '\\nAvg Ensemble Acc: ' + str(avg_ensemble_acc) + \"/\" + str(test_size)+\n",
    "  '\\nMax Ensemble Acc: ' + str(max_ensemble_acc) + \"/\" + str(test_size)+\n",
    "  '\\nModel 1 Acc: ' + str(model1_acc) + \"/\" + str(test_size) +\n",
    "  '\\nModel 2 Acc: ' + str(model2_acc) + \"/\" + str(test_size) +\n",
    "  '\\nModel 3 Acc: ' + str(model3_acc) + \"/\" + str(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=test_x1, y=test_y1, verbose=1)\n",
    "model1.evaluate(x=test_x1, y=test_y1, verbose=1)\n",
    "model2.evaluate(x=test_x2, y=test_y2, verbose=1)\n",
    "model3.evaluate(x=test_x3, y=test_y3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model1.layers[0].get_weights()[0]\n",
    "biases = model1.layers[0].get_weights()[1]\n",
    "\n",
    "weights = model2.layers[0].get_weights()[0]\n",
    "biases = model2.layers[0].get_weights()[1]\n",
    "\n",
    "weights = model3.layers[0].get_weights()[0]\n",
    "biases = model3.layers[0].get_weights()[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
